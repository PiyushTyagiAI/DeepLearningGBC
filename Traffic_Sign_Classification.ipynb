{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b6e13d-dc69-465b-bc93-1870f08546e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c1f561-f849-497a-8eb4-564abe43813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pickle-mixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d2564-a647-4968-b43e-85f0a8774e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13c1c3c-2490-4e5b-84c9-7b113a0f028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing lib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9881790d-4b8a-457e-85f2-c91cf0c22229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting path for the data\n",
    "\n",
    "path = 'C:\\\\Users\\Piyush Tyagi\\OneDrive\\Desktop\\Python notebook\\german_traffice_sign\\myData\\myData'\n",
    "labelFile = 'C:\\\\Users\\Piyush Tyagi\\OneDrive\\Desktop\\Python notebook\\german_traffice_sign\\labels\\labels.csv' # file with all names of classes\n",
    "batch_size_val=50  # how many to process together\n",
    "steps_per_epoch_val=2000\n",
    "epochs_val=10\n",
    "imageDimesions = (32,32,3)\n",
    "testRatio = 0.2    # if 1000 images split will 200 for testing\n",
    "validationRatio = 0.2 # if 1000 images 20% of remaining 800 will be 160 for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6487e2f3-641f-434b-859d-859b6e5fa1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing of the Images\n",
    "count = 0\n",
    "images = []\n",
    "classNo = []\n",
    "myList = os.listdir(path)\n",
    "print(\"Total Classes Detected:\",len(myList))\n",
    "noOfClasses=len(myList)\n",
    "print(\"Importing Classes.....\")\n",
    "for x in range (0,len(myList)):\n",
    "    myPicList = os.listdir(path+\"/\"+str(count))\n",
    "    for y in myPicList:\n",
    "        curImg = cv2.imread(path+\"/\"+str(count)+\"/\"+y)\n",
    "        images.append(curImg)\n",
    "        classNo.append(count)\n",
    "    print(count, end =\" \")\n",
    "    count +=1\n",
    "print(\" \")\n",
    "images = np.array(images)\n",
    "classNo = np.array(classNo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6d9c8-908d-4098-9d8a-c47506b5d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, classNo, test_size=testRatio)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validationRatio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55659473-141e-437d-946d-5776af0cbe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the Data\n",
    "print(\"Data Shapes\")\n",
    "print(\"Train\",end = \"\");print(X_train.shape,y_train.shape)\n",
    "print(\"Validation\",end = \"\");print(X_validation.shape,y_validation.shape)\n",
    "print(\"Test\",end = \"\");print(X_test.shape,y_test.shape)\n",
    "assert(X_train.shape[0]==y_train.shape[0]), \"The number of images in not equal to the number of lables in training set\"\n",
    "assert(X_validation.shape[0]==y_validation.shape[0]), \"The number of images in not equal to the number of lables in validation set\"\n",
    "assert(X_test.shape[0]==y_test.shape[0]), \"The number of images in not equal to the number of lables in test set\"\n",
    "assert(X_train.shape[1:]==(imageDimesions)),\" The dimesions of the Training images are wrong \"\n",
    "assert(X_validation.shape[1:]==(imageDimesions)),\" The dimesionas of the Validation images are wrong \"\n",
    "assert(X_test.shape[1:]==(imageDimesions)),\" The dimesionas of the Test images are wrong\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4505b6db-d3a6-4b55-a981-4bf2f825a7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ CSV FILE\n",
    "data=pd.read_csv(labelFile)\n",
    "print(\"data shape \",data.shape,type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cac7efd-95a9-46bd-9be6-ca299bc3ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISPLAY SOME SAMPLES IMAGES  OF ALL THE CLASSES\n",
    "from matplotlib import cm\n",
    "num_of_samples = []\n",
    "cols = 5\n",
    "num_classes = noOfClasses\n",
    "fig, axs = plt.pyplot.subplots(nrows=num_classes, ncols=cols, figsize=(5, 300))\n",
    "fig.tight_layout()\n",
    "for i in range(cols):\n",
    "    for j,row in data.iterrows():\n",
    "        x_selected = X_train[y_train == j]\n",
    "        axs[j][i].imshow(x_selected[random.randint(0, len(x_selected)- 1), :, :], cmap=cm.get_cmap(\"gray\"))\n",
    "        axs[j][i].axis(\"off\")\n",
    "        if i == 2:\n",
    "            axs[j][i].set_title(str(j)+ \"-\"+row[\"Name\"])\n",
    "            num_of_samples.append(len(x_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edbabba-0434-4f69-8548-4312d9073f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING THE IMAGES\n",
    " \n",
    "def grayscale(img):\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    return img\n",
    "def equalize(img):\n",
    "    img =cv2.equalizeHist(img)\n",
    "    return img\n",
    "def preprocessing(img):\n",
    "    img = grayscale(img)     # CONVERT TO GRAYSCALE\n",
    "    img = equalize(img)      # STANDARDIZE THE LIGHTING IN AN IMAGE\n",
    "    img = img/255            # TO NORMALIZE VALUES BETWEEN 0 AND 1 INSTEAD OF 0 TO 255\n",
    "    return img\n",
    " \n",
    "X_train=np.array(list(map(preprocessing,X_train)))  # TO IRETATE AND PREPROCESS ALL IMAGES\n",
    "X_validation=np.array(list(map(preprocessing,X_validation)))\n",
    "X_test=np.array(list(map(preprocessing,X_test)))\n",
    "cv2.imshow(\"GrayScale Images\",X_train[random.randint(0,len(X_train)-1)]) # TO CHECK IF THE TRAINING IS DONE PROPERLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d054084-e59e-4fa2-bead-782b6ab5491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD A DEPTH OF 1\n",
    "X_train=X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[2],1)\n",
    "X_validation=X_validation.reshape(X_validation.shape[0],X_validation.shape[1],X_validation.shape[2],1)\n",
    "X_test=X_test.reshape(X_test.shape[0],X_test.shape[1],X_test.shape[2],1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e412550-b7f2-4795-973f-7914e8d1e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUGMENTATAION OF IMAGES: TO MAKEIT MORE GENERIC\n",
    "dataGen= ImageDataGenerator(width_shift_range=0.1,   # 0.1 = 10%     IF MORE THAN 1 E.G 10 THEN IT REFFERS TO NO. OF  PIXELS EG 10 PIXELS\n",
    "                            height_shift_range=0.1,\n",
    "                            zoom_range=0.2,  # 0.2 MEANS CAN GO FROM 0.8 TO 1.2\n",
    "                            shear_range=0.1,  # MAGNITUDE OF SHEAR ANGLE\n",
    "                            rotation_range=10)  # DEGREES\n",
    "dataGen.fit(X_train)\n",
    "batches= dataGen.flow(X_train,y_train,batch_size=20)  # REQUESTING DATA GENRATOR TO GENERATE IMAGES  BATCH SIZE = NO. OF IMAGES CREAED EACH TIME ITS CALLED\n",
    "X_batch,y_batch = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb70c308-445b-4018-840c-319cba5a9b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    " #TO SHOW AGMENTED IMAGE SAMPLES\n",
    "fig,axs=plt.pyplot.subplots(1,15,figsize=(20,5))\n",
    "fig.tight_layout()\n",
    " \n",
    "for i in range(15):\n",
    "    axs[i].imshow(X_batch[i].reshape(imageDimesions[0],imageDimesions[1]))\n",
    "    axs[i].axis('off')\n",
    "plt.show()\n",
    " \n",
    " \n",
    "y_train = to_categorical(y_train,noOfClasses)\n",
    "y_validation = to_categorical(y_validation,noOfClasses)\n",
    "y_test = to_categorical(y_test,noOfClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a2682-e171-40d7-a3f3-bca49836c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVOLUTION NEURAL NETWORK MODEL\n",
    "def myModel():\n",
    "    no_Of_Filters=60\n",
    "    size_of_Filter=(5,5) # THIS IS THE KERNEL THAT MOVE AROUND THE IMAGE TO GET THE FEATURES.\n",
    "                         # THIS WOULD REMOVE 2 PIXELS FROM EACH BORDER WHEN USING 32 32 IMAGE\n",
    "    size_of_Filter2=(3,3)\n",
    "    size_of_pool=(2,2)  # SCALE DOWN ALL FEATURE MAP TO GERNALIZE MORE, TO REDUCE OVERFITTING\n",
    "    no_Of_Nodes = 500   # NO. OF NODES IN HIDDEN LAYERS\n",
    "    model= Sequential()\n",
    "    model.add((Conv2D(no_Of_Filters,size_of_Filter,input_shape=(imageDimesions[0],imageDimesions[1],1),activation='relu')))  # ADDING MORE CONVOLUTION LAYERS = LESS FEATURES BUT CAN CAUSE ACCURACY TO INCREASE\n",
    "    model.add((Conv2D(no_Of_Filters, size_of_Filter, activation='relu')))\n",
    "    model.add(MaxPooling2D(pool_size=size_of_pool)) # DOES NOT EFFECT THE DEPTH/NO OF FILTERS\n",
    " \n",
    "    model.add((Conv2D(no_Of_Filters//2, size_of_Filter2,activation='relu')))\n",
    "    model.add((Conv2D(no_Of_Filters // 2, size_of_Filter2, activation='relu')))\n",
    "    model.add(MaxPooling2D(pool_size=size_of_pool))\n",
    "    model.add(Dropout(0.5))\n",
    " \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(no_Of_Nodes,activation='relu'))\n",
    "    model.add(Dropout(0.5)) # INPUTS NODES TO DROP WITH EACH UPDATE 1 ALL 0 NONE\n",
    "    model.add(Dense(noOfClasses,activation='softmax')) # OUTPUT LAYER\n",
    "    # COMPILE MODEL\n",
    "    model.compile(Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e369e10-9f79-44cd-98b7-79073d6b566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "model = myModel()\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "history=model.fit(dataGen.flow(X_train,y_train ,batch_size=50),steps_per_epoch=200,epochs=epochs_val,validation_data=(X_validation,y_validation),verbose=True,shuffle=1)\n",
    " \n",
    "############################### PLOT\n",
    "plt.pyplot.figure(1)\n",
    "plt.pyplot.plot(history.history['loss'])\n",
    "plt.pyplot.plot(history.history['val_loss'])\n",
    "plt.pyplot.legend(['training','validation'])\n",
    "plt.pyplot.title('loss')\n",
    "plt.pyplot.xlabel('epoch')\n",
    "plt.pyplot.figure(2)\n",
    "plt.pyplot.plot(history.history['accuracy'])\n",
    "plt.pyplot.plot(history.history['val_accuracy'])\n",
    "plt.pyplot.legend(['training','validation'])\n",
    "plt.pyplot.title('Acurracy')\n",
    "plt.pyplot.xlabel('epoch')\n",
    "plt.pyplot.show()\n",
    "score =model.evaluate(X_test,y_test,verbose=0)\n",
    "print('Test Score:',score[0])\n",
    "print('Test Accuracy:',score[1])\n",
    " \n",
    " \n",
    "# STORE THE MODEL AS A PICKLE OBJECT\n",
    "pickle_out= open(\"C:\\\\Users\\Piyush Tyagi\\OneDrive\\Desktop\\Python notebook\\german_traffice_sign\\model_trained.p\",\"wb\")  # wb = WRITE BYTE\n",
    "pickle.dump(model,pickle_out)\n",
    "pickle_out.close()\n",
    "cv2.waitKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf88854e-efdf-4ccc-b0ae-a0a1e5ca2b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## Testing Code\n",
    " \n",
    "frameWidth= 640         # CAMERA RESOLUTION\n",
    "frameHeight = 480\n",
    "brightness = 180\n",
    "threshold = 0.75         # PROBABLITY THRESHOLD\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "##############################################\n",
    " \n",
    "# SETUP THE VIDEO CAMERA\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, frameWidth)\n",
    "cap.set(4, frameHeight)\n",
    "cap.set(10, brightness)\n",
    "# IMPORT THE TRANNIED MODEL\n",
    "pickle_in=open(\"C:\\\\Users\\Piyush Tyagi\\OneDrive\\Desktop\\Python notebook\\german_traffice_sign\\model_trained.p\",\"rb\")  ## rb = READ BYTE\n",
    "model=history\n",
    " \n",
    "def grayscale(img):\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    return img\n",
    "def equalize(img):\n",
    "    img =cv2.equalizeHist(img)\n",
    "    return img\n",
    "def preprocessing(img):\n",
    "    img = grayscale(img)\n",
    "    img = equalize(img)\n",
    "    img = img/255\n",
    "    return img\n",
    "def getCalssName(classNo):\n",
    "    if   classNo == 0: return 'Speed Limit 20 km/h'\n",
    "    elif classNo == 1: return 'Speed Limit 30 km/h'\n",
    "    elif classNo == 2: return 'Speed Limit 50 km/h'\n",
    "    elif classNo == 3: return 'Speed Limit 60 km/h'\n",
    "    elif classNo == 4: return 'Speed Limit 70 km/h'\n",
    "    elif classNo == 5: return 'Speed Limit 80 km/h'\n",
    "    elif classNo == 6: return 'End of Speed Limit 80 km/h'\n",
    "    elif classNo == 7: return 'Speed Limit 100 km/h'\n",
    "    elif classNo == 8: return 'Speed Limit 120 km/h'\n",
    "    elif classNo == 9: return 'No passing'\n",
    "    elif classNo == 10: return 'No passing for vechiles over 3.5 metric tons'\n",
    "    elif classNo == 11: return 'Right-of-way at the next intersection'\n",
    "    elif classNo == 12: return 'Priority road'\n",
    "    elif classNo == 13: return 'Yield'\n",
    "    elif classNo == 14: return 'Stop'\n",
    "    elif classNo == 15: return 'No vechiles'\n",
    "    elif classNo == 16: return 'Vechiles over 3.5 metric tons prohibited'\n",
    "    elif classNo == 17: return 'No entry'\n",
    "    elif classNo == 18: return 'General caution'\n",
    "    elif classNo == 19: return 'Dangerous curve to the left'\n",
    "    elif classNo == 20: return 'Dangerous curve to the right'\n",
    "    elif classNo == 21: return 'Double curve'\n",
    "    elif classNo == 22: return 'Bumpy road'\n",
    "    elif classNo == 23: return 'Slippery road'\n",
    "    elif classNo == 24: return 'Road narrows on the right'\n",
    "    elif classNo == 25: return 'Road work'\n",
    "    elif classNo == 26: return 'Traffic signals'\n",
    "    elif classNo == 27: return 'Pedestrians'\n",
    "    elif classNo == 28: return 'Children crossing'\n",
    "    elif classNo == 29: return 'Bicycles crossing'\n",
    "    elif classNo == 30: return 'Beware of ice/snow'\n",
    "    elif classNo == 31: return 'Wild animals crossing'\n",
    "    elif classNo == 32: return 'End of all speed and passing limits'\n",
    "    elif classNo == 33: return 'Turn right ahead'\n",
    "    elif classNo == 34: return 'Turn left ahead'\n",
    "    elif classNo == 35: return 'Ahead only'\n",
    "    elif classNo == 36: return 'Go straight or right'\n",
    "    elif classNo == 37: return 'Go straight or left'\n",
    "    elif classNo == 38: return 'Keep right'\n",
    "    elif classNo == 39: return 'Keep left'\n",
    "    elif classNo == 40: return 'Roundabout mandatory'\n",
    "    elif classNo == 41: return 'End of no passing'\n",
    "    elif classNo == 42: return 'End of no passing by vechiles over 3.5 metric tons'\n",
    " \n",
    "while True:\n",
    " \n",
    "# READ IMAGE\n",
    "    success, imgOrignal = cap.read()\n",
    " \n",
    "# PROCESS IMAGE\n",
    "img = np.asarray(imgOrignal)\n",
    "img = cv2.resize(img, (32, 32))\n",
    "img = preprocessing(img)\n",
    "cv2.imshow(\"Processed Image\", img)\n",
    "img = img.reshape(1, 32, 32, 1)\n",
    "cv2.putText(imgOrignal, \"CLASS: \" , (20, 35), font, 0.75, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "cv2.putText(imgOrignal, \"PROBABILITY: \", (20, 75), font, 0.75, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "# PREDICT IMAGE\n",
    "predictions = model.predict(img)\n",
    "classIndex = model.predict_classes(img)\n",
    "probabilityValue =np.amax(predictions)\n",
    "if probabilityValue > threshold:\n",
    "#print(getCalssName(classIndex))\n",
    "    cv2.putText(imgOrignal,str(classIndex)+\" \"+str(getCalssName(classIndex)), (120, 35), font, 0.75, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    cv2.putText(imgOrignal, str(round(probabilityValue*100,2) )+\"%\", (180, 75), font, 0.75, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    cv2.imshow(\"Result\", imgOrignal)\n",
    " \n",
    "if cv2.waitKey(1) and 0xFF == ord('q'):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac8a13-5bf4-42f4-a6ae-5aece8afc979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
